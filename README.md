# AoC Agent

An autonomous AI agent designed to solve Advent of Code (AoC) problems using Large Language Models (LLMs).

## Project Goal

The goal of this project is to build a simple yet extendable AI agent that can automatically solve AoC puzzles. It serves as an experimental playground to:
1.  Gain experience with agentic systems and tool use.
2.  Compare the performance of different LLMs (Gemini, OpenAI).
3.  Compare code generation quality across different programming languages (Python, Kotlin, C#).

## Architecture

The project is structured as follows:

*   **`src/aoc_agent/cli.py`**: The command-line entry point. It parses arguments and initializes the runner.
*   **`src/aoc_agent/agent/agent_runner.py`**: Orchestrates multiple runs based on the specified configuration (years, days, languages, models).
*   **`src/aoc_agent/agent/miniagent.py`**: The core agent logic. It initializes the LLM, binds tools, and manages the agent loop (LangChain based).
*   **`src/aoc_agent/agent/tools.py`**: Defines the tools available to the agent:
    *   `DownloadProblemStatement`: Fetches the puzzle description.
    *   `DownloadInput`: Fetches the user-specific puzzle input.
    *   `RunGeneratedCode`: Executes the generated code in a sandbox (supporting Python, Kotlin, C#).
    *   `SubmitAnswer`: Submits the solution to AoC.
*   **`src/aoc_agent/core/`**: Contains core utilities, including the `AocClient` for interacting with the AoC website and language-specific code runners.
*   **`src/aoc_agent/agent/report_builder.py`**: Aggregates run data and generates an HTML report.

## Responsibilities

*   **Agent**: The LLM acts as the reasoning engine. It decides which tools to call based on the problem statement and feedback from previous steps.
*   **Toolbox**: Provides the interface between the agent and the external world (AoC website, local file system, code execution environment).
*   **Runner**: Manages the lifecycle of a solution attempt, enforcing limits and recording metrics.

## Setup & How to Run

### Prerequisites

*   Python 3.10+
*   Poetry (recommended for dependency management)

### API Keys Required

You must set the following environment variables:

1.  **`AOC_SESSION`** (Required): Your Advent of Code session cookie. This is needed to download inputs and submit answers.
2.  **`GOOGLE_API_KEY`** (Required if using Gemini models): API key for Google's Generative AI.
3.  **`OPENAI_API_KEY`** (Required if using OpenAI models): API key for OpenAI.
4.  **`LANGSMITH_API_KEY`** (Optional): If you want to use LangSmith for tracing agent execution.

### Installation

```bash
poetry install
```

### Running the Agent

Use the `aoc-agent` command (or `poetry run aoc-agent`) to start the agent.

**Examples:**

Run for Year 2024, Day 1, using Python and the default model (Gemini Flash):
```bash
poetry run aoc-agent --year 2024 --days 1 --langs python
```

Run for Days 1 through 5, using both Python and Kotlin:
```bash
poetry run aoc-agent --year 2024 --days 1-5 --langs python kotlin
```

Run with a specific model (e.g., OpenAI GPT-4o):
```bash
poetry run aoc-agent --year 2024 --days 1 --models gpt-4o
```

**Arguments:**

*   `--year`: The AoC year (e.g., 2024).
*   `--days`: Specific day(s) or range (e.g., `1`, `1-5`, `1,3,5`).
*   `--langs`: Languages to use (`python`, `kotlin`, `csharp`). Default: `python`.
*   `--models`: LLMs to use (e.g., `gemini-2.5-flash`, `gpt-4o`). Default: `gemini-2.5-flash`.
*   `--repeats`: Number of times to repeat each run configuration.

## Per-Run Final Report

In addition to the aggregate HTML report, the agent generates a `final_report.md` for each successful run. This file is located in the run directory (e.g., `data/run/.../final_report.md`) and contains a comprehensive explanation of the solution, including:

*   **Problem Summary**
*   **Algorithmic Approach (Part 1 & 2)**
*   **Key Insights & "Gotchas"**
*   **Efficiency Analysis**

This report is intended to be educational and provides insight into the agent's reasoning process.

## Explanation of the Report

After the runs complete, the system generates an HTML report in `data/reports/report.html`. This report provides a detailed analysis of the agent's performance.

**Key Metrics in the Report:**

*   **Success Rate**: Percentage of runs where Part 1 and Part 2 were solved.
*   **Duration**: Time taken to solve the problem.
*   **Tokens**: Total number of output tokens generated by the LLM.
*   **Friction**: A metric representing the "struggle" of the agent. It is calculated based on the number of code execution errors, incorrect submissions, and iterations required. Lower friction means a smoother solving process.

**Visualizations:**

*   **Head-to-Head Comparison**: A matrix comparing different languages on the same tasks (e.g., Python vs. Kotlin). It shows relative time, token usage, and friction.
*   **Token Usage Charts**: Scatter plots comparing token usage between languages.
*   **Detailed Stats Table**: A comprehensive table showing metrics for every combination of Day, Language, and Model.
